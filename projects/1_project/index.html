<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> DOG: Dynamic Object Grasping | Saurav Dosi </title> <meta name="author" content="Saurav Dosi"> <meta name="description" content="Robotic grasping of object moving on 2D Deterministic Path with real-time Recursive Least Squares motion prediction."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?3f1501664d26a5259965a84120e8d92f"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sauravdosi.github.io/projects/1_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Saurav</span> Dosi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/experience/">experience </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">DOG: Dynamic Object Grasping</h1> <p class="post-description">Robotic grasping of object moving on 2D Deterministic Path with real-time Recursive Least Squares motion prediction.</p> </header> <article> <section id="badgeproj-section"> <h2 class="badgeproj-title">Tech Stack 💻 &amp; Resources📚</h2> <div class="badgeproj-container"> <span class="badgeproj">Robot Manipulation</span> <span class="badgeproj">ROS</span> <span class="badgeproj">Gazebo</span> <span class="badgeproj">PyBullet</span> <span class="badgeproj">NumPy</span> </div> <div class="linksproj-container"> <a href="https://github.com/adityavkulkarni/2D_dynamic_object_grasping" target="_blank" class="linkproj" rel="external nofollow noopener"> <i class="fab fa-github"></i> GitHub Repository </a> <a href="https://drive.google.com/file/d/16bMPWcBXGA10P0gyKAaFOmN-yCimzDyW/view?usp=sharing" target="_blank" class="linkproj" rel="external nofollow noopener"> <i class="fas fa-file-alt"></i> Report </a> <a href="https://drive.google.com/file/d/1cyYIJvOVgrl79BqiMGc6voBraZUoIiu_/view?usp=sharing" target="_blank" class="linkproj" rel="external nofollow noopener"> <i class="fas fa-file-powerpoint"></i> Slides </a> <a href="https://utdallas.app.box.com/s/tb7logwkyp9kxky6ieu988e71gn74qph" target="_blank" class="linkproj" rel="external nofollow noopener"> <i class="fas fa-video"></i> Demo Video </a> </div> </section> <p><strong>Authors</strong>: <a href="https://sauravdosi.github.io/">Saurav Dosi</a>, <a href="https://adityavkulkarni.github.io/" rel="external nofollow noopener" target="_blank">Aditya Kulkarni</a>, and <a href="https://www.linkedin.com/in/ferozhatha/" rel="external nofollow noopener" target="_blank">Feroz Hatha</a></p> <p><strong>Course</strong>: CS 6301 - Robotics, taught by <a href="https://yuxng.github.io/" rel="external nofollow noopener" target="_blank">Dr. Yu Xiang</a>, The University of Texas at Dallas</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog/dog-480.webp 480w,/assets/img/dog/dog-800.webp 800w,/assets/img/dog/dog-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/dog/dog.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Project Demo GIF. </div> <h2 id="1-abstract">1. Abstract</h2> <p>Robotic grasping has always been a fascinating challenge—especially when it comes to grabbing objects on the move. In this project, we tackled the problem of efficiently and accurately grasping a dynamic object traveling along a predictable (deterministic) path.</p> <p>Our solution involves designing a control system that predicts the object’s future pose, calculates the optimal interception point, and ensures precise grasping—all while accounting for trajectory feedback and timing constraints.</p> <h2 id="2-introduction">2. Introduction</h2> <p>Grasping objects in motion is no easy task, particularly when the object follows a deterministic path. Yet, this problem is highly relevant in industrial robotics, where manipulators need to pick up items moving on conveyor belts or perform mobile manipulations in dynamic environments.</p> <p>To tackle this challenge, we developed a control system that enables a robotic manipulator to adapt to the future positions of a moving object. By interpreting the object’s trajectory in real time, the robot can plan its motion to intercept and grasp the object with precision.</p> <h4 id="21-why-this-matters">2.1. Why This Matters</h4> <p>Dynamic object grasping combines key aspects of robotics: perception, planning, and control. Mastering this opens doors to solving real-world industrial problems, such as:</p> <ul> <li>Picking up objects from conveyor belts.</li> <li>Mobile robots interacting with static/moving environments.</li> </ul> <h4 id="22-inspiration-and-methodology">2.2. Inspiration and Methodology</h4> <p>Our approach builds on the Grasping Trajectory Optimization with Point Clouds (GraspTrajOpt) technique by Xiang et al., originally designed for static objects. We extended this for dynamic scenarios by:</p> <ol> <li>Predicting future object poses.</li> <li>Optimizing the robot’s trajectory for interception. The Fetch robot, paired with ROS and PyBullet simulations, served as our testbed, offering a robust platform to test and refine our ideas.</li> </ol> <h4 id="23-a-progressive-approach">2.3. A Progressive Approach</h4> <p>The project unfolds across three phases:</p> <ol> <li> <strong>Linear Motion</strong>: Simplifying the problem with objects moving linearly (like on a conveyor belt).</li> <li> <strong>Complex Trajectories</strong>: Expanding to circular and sinusoidal paths, integrating kinematic adjustments.</li> <li> <strong>Perception Integration</strong>: Using an egocentric camera to detect object positions and model their motion dynamically.</li> </ol> <h2 id="3-related-work">3. Related Work</h2> <p>Dynamic object grasping has been an active area of research, with numerous approaches tackling different aspects of this challenge. Here’s a look at some key contributions that inspired and guided our work:</p> <h4 id="31-reachability-aware-grasping">3.1. Reachability-Aware Grasping</h4> <p>Akinola, Xu, and colleagues presented a dynamic grasping framework designed to account for both reachability and motion~\cite{akinola2021dynamic}. They introduced:</p> <ul> <li> <strong>Signed Distance Fields</strong>: Used to model the robot’s reachability space, making it easier to eliminate unreachable grasps quickly.</li> <li> <strong>Neural Network Predictions</strong>: A model predicts the quality of potential grasps based on the object’s motion, enabling real-time filtering of grasp options.</li> <li> <strong>Motion Stability</strong>: By using solutions from the previous time step, the system generates new arm trajectories close to prior plans, avoiding instability or fluctuation.</li> </ul> <p>The framework leverages a recurrent neural network (RNN) to model and predict object motion, creating a robust and adaptable grasping strategy.</p> <h4 id="32-human-to-robot-handovers">3.2. Human-to-Robot Handovers</h4> <p>Robotic systems often struggle with the diverse sizes, shapes, and deformability of objects during handovers. Yang et al.~\cite{yangreact2020} tackled this problem by developing a vision-based system for reactive handovers, focusing on:</p> <ul> <li> <strong>Closed-Loop Motion Planning</strong>: Ensures smooth and reactive grasping, even when objects are positioned unpredictably.</li> <li> <strong>Real-Time Grasp Generation</strong>: Adapts dynamically to rigid and non-rigid objects alike.</li> </ul> <p>Their approach demonstrated exceptional robustness, tested on 26 diverse household objects and further validated through user studies.</p> <h4 id="33-learning-to-grasp-dynamically">3.3. Learning to Grasp Dynamically</h4> <p>Wu’s work~\cite{wu2022grasparl} took a different angle by introducing an adversarial reinforcement learning framework called GraspARL. This method frames the dynamic grasping challenge as a “move-and-grasp” game, where:</p> <ul> <li>The robot attempts to grasp the object.</li> <li>An adversarial mover tries to escape, simulating complex, unpredictable trajectories.</li> </ul> <p>By auto-generating diverse motion patterns, this framework enhances the robot’s generalization capabilities. The results? Improved performance in both simulations and real-world scenarios.</p> <h4 id="34-insights-for-our-approach">3.4. Insights for Our Approach</h4> <p>These studies provided valuable insights, such as using predictive models for motion-aware grasping and leveraging reinforcement learning for adaptability. Building on these principles, our project aims to develop a streamlined, deterministic approach that bridges the gap between trajectory prediction and efficient grasping.</p> <h2 id="method">Method</h2> <h4 id="41-simulation-setup">4.1. Simulation Setup</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog/6301-480.webp 480w,/assets/img/dog/6301-800.webp 800w,/assets/img/dog/6301-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/dog/6301.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Workflow diagram of the architecture. </div> <p>To simulate a robotic manipulation task involving a Fetch robot, we utilized Robot Operating System (ROS) integrated with Gazebo. The simulation environment consisted of a Fetch mobile manipulator robot, a table or conveyor belt, and a cube moving on the table/conveyor belt. The primary objective of Phase 1 was to intercept and grasp the moving cube using the robot’s arm.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog/dog1-480.webp 480w,/assets/img/dog/dog1-800.webp 800w,/assets/img/dog/dog1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/dog/dog1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog/dog2-480.webp 480w,/assets/img/dog/dog2-800.webp 800w,/assets/img/dog/dog2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/dog/dog2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog/dog3-480.webp 480w,/assets/img/dog/dog3-800.webp 800w,/assets/img/dog/dog3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/dog/dog3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> On the left, is the simple_grasp setup from fetch_gazebo repository. Middle, has a longer table. Right, is the final setup with a moving conveyor belt. </div> <h5 id="411-table-setup-with-uniform-rectilinear-motion">4.1.1. Table Setup with Uniform Rectilinear Motion</h5> <p>Initially, the cube was programmed to move in a straight line on a table surface at a constant velocity.</p> <ul> <li> <p><strong>Implementation Details</strong>:</p> <ul> <li>A Python script was developed to create a ROS node, publishing to the <code class="language-plaintext highlighter-rouge">/gazebo/set_model_state</code> topic.</li> <li>The cube’s initial position and velocity (0.01 m/s) were defined, with its position updated iteratively in the y-axis. The script ensured the cube reset to its initial position once it reached the table’s edge.</li> </ul> </li> <li> <p><strong>Challenges</strong>:</p> <ul> <li>The default table length in the fetch_gazebo package was insufficient for dynamic grasping tasks.</li> <li>High-frequency oscillations were observed due to teleportation-based movement, conflicting with Gazebo’s physics engine.</li> <li>The cube’s velocity persisted after grasping, causing grip instability.</li> </ul> </li> </ul> <h5 id="412-table-replacement-with-extended-urdf-setup">4.1.2. Table Replacement with Extended URDF Setup</h5> <p>To address the short table issue, a custom URDF file for a longer table was created.</p> <ul> <li> <strong>Configuration</strong>: <ul> <li>The URDF defined the table’s material properties, dimensions, and fixed joints.</li> <li>The longer table was integrated into the Gazebo environment by modifying the launch file.</li> </ul> </li> </ul> <p>Despite mitigating oscillation issues by adjusting friction coefficients, complete resolution was unattainable due to the limitations of teleportation-based motion.</p> <h5 id="413-conveyor-belt-setup">4.1.3. Conveyor Belt Setup</h5> <p>To overcome the motion and grip issues, a conveyor belt plugin was used to simulate the cube’s movement.</p> <ul> <li> <strong>Advantages</strong>: <ul> <li>Cube motion was governed by the conveyor surface instead of direct velocity application.</li> <li>Conveyor speed was adjustable via ROS service calls, ensuring synchronization with the robot’s grasping task.</li> </ul> </li> </ul> <h4 id="42-motion-planning-and-grasping">4.2. Motion Planning and Grasping</h4> <p>The robot’s trajectory planning was divided into two steps:</p> <ul> <li> <strong>Horizontal Alignment</strong>: <ul> <li>The robot moved horizontally to align 0.5 units above the predicted intercept point on the y-axis.</li> </ul> </li> <li> <strong>Vertical Descent and Grasping</strong>: <ul> <li>After alignment, the robot descended vertically to grasp and pick up the cube.</li> </ul> </li> <li> <strong>Verification</strong>: <ul> <li>Successful grasps were verified by checking the cube’s z-coordinate after lifting, ensuring it was above the table’s surface.</li> </ul> </li> <li> <strong>Integration with MoveIt</strong>: <ul> <li>MoveIt was utilized for motion planning and collision-free trajectory generation.</li> <li>Time parameterization tools in MoveIt enabled synchronization between the robot’s motion and the cube’s velocity.</li> </ul> </li> </ul> <h4 id="43-evaluation">4.3 Evaluation</h4> <p>During Phase 1 testing, the cube was moved linearly at a velocity of 0.1 units along the y-axis.</p> <ul> <li>Initial intercept coordinates were manually determined and updated dynamically after each successful grasp.</li> <li>The robot’s pre-grasp position and motion planning time were tuned based on the cube’s speed and trajectory, achieving consistent grasping performance.</li> </ul> <p>This method provided a robust framework for addressing dynamic object manipulation in a simulated environment while highlighting practical challenges and solutions.</p> <h4 id="44-trajectory-prediction">4.4 Trajectory Prediction</h4> <p>To predict the trajectory of an object moving on a deterministic path, we use Recursive Least Squares (RLS). We take position coordinates of the moving object \((x,y)\) as input from the Gazebo simulation, update/train the RLS fitter separately for \(x(t)\) and \(y(t)\), and then predict the position for future time steps \(T\). While the fitter can be a polynomial of any degree, we incorporate sines and cosines for circular and sinusoidal motion of the object for future work.</p> <h5 id="441-objective">4.4.1. Objective:</h5> <p>The goal of RLS is to minimize the weighted sum of the squared errors between the predicted output and the actual output. Given a linear model</p> <p>\(y(t) = w^T x(t) + ϵ(t)\) , where:</p> <ul> <li>\(y(t)\) is the observed output at time \(t\).</li> <li>\(w\) is the parameter vector to be estimated.</li> <li>\(x(t)\) is the input vector (features).</li> <li>\(ϵ(t)\)is the error term.</li> </ul> <h5 id="442-error-definition">4.4.2. Error Definition:</h5> <p>The error \(e(t)\) at time \(t\) is defined as:</p> \[e(t) = y(t) - \mathbf{w}^T \mathbf{x}(t)\] <p>The objective is to minimize the cost function:</p> \[J(t) = \sum_{k=0}^{t} \lambda^{t-k} e(k)^2\] <p>where \(λ\) (forgetting factor) controls the weight given to older observations.</p> <h5 id="443-recursive-update-the-key-steps-in-rls-involve">4.4.3. Recursive Update: The key steps in RLS involve:</h5> <ul> <li> <p><strong>Initialization</strong>: Start with an initial guess for the parameter vector \(w(0)\) and an initial covariance matrix \(P(0)\), often chosen to be large to allow flexibility in the initial estimates.</p> </li> <li> <p><strong>Gain Calculation</strong>: For each new observation, compute the gain vector \(K(t):\)</p> </li> </ul> \[\mathbf{K}(t) = \frac{\mathbf{P}(t-1) \mathbf{x}(t)}{\lambda + \mathbf{x}(t)^T \mathbf{P}(t-1) \mathbf{x}(t)}\] <p>This gain vector indicates how much to adjust the parameters based on the current error.</p> <ul> <li> <strong>Parameter Update</strong>: Update the parameter vector:</li> </ul> \[\mathbf{w}(t) = \mathbf{w}(t-1) + \mathbf{K}(t) e(t)\] <ul> <li> <strong>Covariance Matrix Update</strong>: Update the covariance matrix:</li> </ul> \[\mathbf{P}(t) = \frac{1}{\lambda} \left( \mathbf{P}(t-1) - \mathbf{K}(t) \mathbf{x}(t)^T \mathbf{P}(t-1) \right)\] <h4 id="45-intercepting-a-moving-object">4.5. Intercepting a Moving Object</h4> <p>Intercepting a moving object to grasp it while it moves along a deterministic path is tricky. This involves considering the time needed for Inverse Kinematics (IK), executing the IK solution, and closing the gripper to pick the object. Here, we developed an algorithm to predict the best time to intercept a moving cube on the table along the y-axis.</p> <h5 id="451-intercept-algorithm">4.5.1. Intercept Algorithm:</h5> <p>We used a recursive least squares (RLS) approach to predict the cube’s trajectory. The algorithm involves sampling future positions of the cube, calculating the required time for the gripper to reach the target, and then selecting the closest reachable point where the IK solution can be executed.</p> <p>This allows the robot to intercept the cube at the correct time by aligning the gripper with the predicted position and grabbing it before it moves out of reach. Fine-tuning was required, especially due to uncertainties in the time needed for IK solutions.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0" style="max-width: 500px; margin: auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog/doga4-480.webp 480w,/assets/img/dog/doga4-800.webp 800w,/assets/img/dog/doga4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/dog/doga4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Interception Algorithm. </div> <h2 id="5-experiments">5. Experiments</h2> <h4 id="51-pre-grasp-to-grasp-time">5.1. Pre-grasp to Grasp Time</h4> <p>We measured the time taken for the robot to move from the pre-grasp position to the grasp position. As the cube moved along the y-axis, we saw that it became more challenging for the robot to reach the target when the cube moved lower down the table. A sharp increase in time was observed when the cube reached lower y-coordinates.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog/grasp_pose_estimate_time80-480.webp 480w,/assets/img/dog/grasp_pose_estimate_time80-800.webp 800w,/assets/img/dog/grasp_pose_estimate_time80-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/dog/grasp_pose_estimate_time80.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Distribution of duration for pre-grasp to grasp position movement. </div> <p><strong>Key Observations</strong>:</p> <ul> <li>From iterations 0 to 30, the robot’s efficiency in reaching the cube improved, requiring less time.</li> <li>Beyond iteration 30, the time to grasp increased due to the robot’s limitations in reaching lower positions on the y-axis.</li> <li>A significant spike in time occurred at iteration 70 when the cube reached near the table’s edge.</li> </ul> <h5 id="511-estimated-vs-actual-time">5.1.1. Estimated vs Actual Time</h5> <p>We also measured the offset between the estimated and actual time taken to execute a trajectory. Generally, the offset remained small, except for a sharp spike at iteration 70, when the robot had to reconfigure its joints for a more complex grasp.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog/offset80-480.webp 480w,/assets/img/dog/offset80-800.webp 800w,/assets/img/dog/offset80-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/dog/offset80.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Difference between MoveIt estimate and actual movement duration. </div> <h4 id="52-gazebo-time-vs-real-world-time">5.2. Gazebo Time vs Real-World Time</h4> <p>We compared the solution calculation time in Gazebo with real-world performance. In general, Gazebo and real-world times followed similar trends, with noticeable spikes in computation time at certain points. However, real-world execution consistently took longer during these spikes.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog/solution_time_sim_real80-480.webp 480w,/assets/img/dog/solution_time_sim_real80-800.webp 800w,/assets/img/dog/solution_time_sim_real80-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/dog/solution_time_sim_real80.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Intercept solution calculation duration trend. </div> <h4 id="53-evaluation">5.3. Evaluation</h4> <p>We conducted 25 iterations to evaluate grasp success and time:</p> <ul> <li> <strong>Grasp Success Rate</strong>: The system successfully grasped the cube in 24 out of 25 attempts, achieving a 75% success rate.</li> <li> <strong>Grasp Time</strong>: The average grasp time was 7.2 seconds across all successful iterations.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog/total_time25-480.webp 480w,/assets/img/dog/total_time25-800.webp 800w,/assets/img/dog/total_time25-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/dog/total_time25.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Total actual Grasp time. </div> <p>This evaluation confirms the effectiveness of the interception algorithm in simulating the grasping task.</p> <h2 id="6-limitations-and-future-work">6. Limitations and Future Work</h2> <h4 id="61-perception">6.1. Perception</h4> <p>Currently, we rely on Gazebo for object pose estimation. In the future, we aim to improve the perception system by using the robot’s egocentric camera for real-time object tracking. This will involve RGBD data, object detection/segmentation, and computational geometry to track the object’s position relative to the depth camera. Integrating this with our current logic will improve grasping accuracy and efficiency, particularly for dynamic objects with non-linear paths or varying velocities.</p> <h4 id="62-exploring-non-linear-paths">6.2. Exploring Non-linear Paths</h4> <p>We plan to extend our system to handle objects moving along non-linear paths, such as circular or sinusoidal trajectories. By using real-time sensor data and Recursive Least Squares (RLS) for trajectory prediction, we’ll incorporate periodic functions like sine and cosine to accurately predict object positions along these complex paths.</p> <h4 id="63-reinforcement-learning">6.3. Reinforcement Learning</h4> <p>We are considering the use of Reinforcement Learning (RL) to refine our interception logic. With RL, the system can learn a reward function based on grasp quality and interception success, automating fine-tuning and reducing human effort.</p> <h4 id="64-mobile-manipulation">6.4. Mobile Manipulation</h4> <p>We also plan to apply the system to mobile manipulation, where the robot approaches an object at a constant velocity, planning and executing the grasp while in motion. This will make the process more efficient by reducing time delays in adjusting to the object’s position.</p> <h2 id="7-conclusion">7. Conclusion</h2> <p>This study demonstrates the successful implementation of robotic grasping for dynamic objects moving along deterministic paths, achieving an 80% grasp success rate with the Fetch robot in simulated environments. By integrating trajectory prediction with Recursive Least Squares and effective control strategies, we’ve created a robust system for grasping in dynamic scenarios.</p> <p>Our experiments provide valuable insights into areas for optimization, such as reducing time delays in grasping. This research has significant implications for automated systems like conveyor belt sorting, where precision and efficiency are essential.</p> <p>Looking ahead, we plan to enhance the system’s adaptability by exploring non-linear object paths, improving perception techniques, and developing more sophisticated control strategies. These advancements will contribute to more versatile and efficient automation solutions in dynamic environments.</p> <div style="font-size: 1em; font-weight: bold; text-align: center; margin-bottom: 20px;"> "With each grasp, we're not just reaching for objects, but for the future of smarter, more agile robotics." </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Saurav Dosi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Website inspired from <a href="https://jishnujayakumar.github.io/" target="_blank" rel="external nofollow noopener">Jishnu</a> and <a href="https://ahadjawaid.com/" target="_blank" rel="external nofollow noopener">Ahad</a> at <a href="https://labs.utdallas.edu/irvl/" target="_blank" rel="external nofollow noopener">IRVL</a>. Last updated: January 21, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"Dive into the stories behind my research\u2014papers that push boundaries and ideas that spark conversations. \ud83d\udcda\u2728 (* stands for equal contribution from the authors.)",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-experience",title:"experience",description:"Walk through my career journey, from solving real-world puzzles to crafting innovative solutions that matter. \ud83d\udcbc\ud83d\ude80",section:"Navigation",handler:()=>{window.location.href="/experience/"}},{id:"nav-projects",title:"projects",description:"Explore the labors of love and curiosity\u2014research breakthroughs and academic explorations that define my path. \ud83d\udd2c\ud83d\udca1",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"Your one-stop snapshot of my journey\u2014skills, achievements, and everything I bring to the table. \ud83d\udcdd\ud83d\udcc8",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-a-post-with-image-galleries",title:"a post with image galleries",description:"this is what included image galleries could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/photo-gallery/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-awarded-jonsson-school-dean-s-scholarship-at-utd-medal-sports",title:'Awarded Jonsson School Dean\u2019s Scholarship at UTD! <img class="emoji" title=":medal_sports:" alt=":medal_sports:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c5.png" height="20" width="20">',description:"",section:"News"},{id:"news-joined-intelligent-robotics-and-vision-lab-as-a-graduate-ai-researcher-robot",title:'Joined Intelligent Robotics and Vision Lab as a Graduate AI Researcher! <img class="emoji" title=":robot:" alt=":robot:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png" height="20" width="20">',description:"",section:"News"},{id:"news-employed-as-a-data-science-nlp-intern-at-isn-dallas-tx-confetti-ball",title:'Employed as a Data Science (NLP) Intern at ISN, Dallas, TX! <img class="emoji" title=":confetti_ball:" alt=":confetti_ball:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f38a.png" height="20" width="20">',description:"",section:"News"},{id:"news-employed-as-a-teaching-assistant-for-cs3345-data-structures-and-algorithms-mortar-board",title:'Employed as a Teaching Assistant for CS3345 - Data Structures and Algorithms! <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20">...',description:"",section:"News"},{id:"projects-stair-keydet-keypoint-detection",title:"Stair KeyDet: Keypoint Detection",description:"Mirrag Keypoint Detection",section:"Projects",handler:()=>{window.location.href="/projects/10_project/"}},{id:"projects-intelligent-name-matching",title:"Intelligent Name Matching",description:"A BiLSTM-Attention model trained to prioritize keywords in a given company name.",section:"Projects",handler:()=>{window.location.href="/projects/11_project/"}},{id:"projects-diffusion-in-navigation-talk",title:"diffusion in navigation talk",description:"A BiLSTM-Attention model trained to prioritize keywords in a given company name.",section:"Projects",handler:()=>{window.location.href="/projects/12_project/"}},{id:"projects-real-world-rl-for-mobile-manip-talk",title:"real world rl for mobile manip talk",description:"A BiLSTM-Attention model trained to prioritize keywords in a given company name.",section:"Projects",handler:()=>{window.location.href="/projects/13_project/"}},{id:"projects-dog-dynamic-object-grasping",title:"DOG: Dynamic Object Grasping",description:"Robotic grasping of object moving on 2D Deterministic Path with real-time Recursive Least Squares motion prediction.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-investaid-ai-powered-investment-dashboard",title:"InvestAID: AI powered Investment Dashboard",description:"",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-navrl-robot-navigation-using-rl",title:"NavRL: Robot Navigation using RL",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-fetchnavisaac-robot-navigation-toolkit",title:"FetchNavIsaac: Robot Navigation Toolkit",description:"An NVIDIA Isaac Sim Navigation starter kit",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-gdbwat-graph-db-watermarking",title:"GDBWat: Graph DB Watermarking",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-language-modelling",title:"Language Modelling",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-rnn-based-sentiment-classification",title:"RNN based Sentiment Classification",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-8-project",title:"8_project",description:"VTON Diffusion",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"BTech Thesis",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%73%61%75%72%61%76.%64%6F%73%69@%75%74%64%61%6C%6C%61%73.%65%64%75","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/sauravdosi","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/sauravdosi","_blank")}},{id:"social-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=fOZjUXgAAAAJ","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>